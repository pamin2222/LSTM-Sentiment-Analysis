{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numDimensions = 300\n",
    "maxSeqLength = 25\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsList = np.load('wordsList.npy').tolist()\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter, word in enumerate(split):\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Mobile Brands Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brand_data_dir = \"twitter_data/BrandMobile_1/\"\n",
    "apple_tweet_path = os.path.join(brand_data_dir, \"Apple.csv\")\n",
    "huawei_tweet_path = os.path.join(brand_data_dir, \"Huawei.csv\")\n",
    "oppo_tweet_path = os.path.join(brand_data_dir, \"Oppo.csv\")\n",
    "samsung_tweet_path = os.path.join(brand_data_dir, \"Samsung.csv\")\n",
    "vivo_tweet_path = os.path.join(brand_data_dir, \"Vivo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 261: ',' expected after '\"'\n"
     ]
    }
   ],
   "source": [
    "apple_tweet_df = pd.read_csv(apple_tweet_path, engine='python')\n",
    "huawei_tweet_df = pd.read_csv(huawei_tweet_path, engine='python', error_bad_lines=False)\n",
    "oppo_tweet_df = pd.read_csv(oppo_tweet_path, engine='python')\n",
    "samsung_tweet_df = pd.read_csv(samsung_tweet_path, engine='python')\n",
    "vivo_tweet_df = pd.read_csv(vivo_tweet_path, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Created-At</th>\n",
       "      <th>From-User</th>\n",
       "      <th>From-User-Id</th>\n",
       "      <th>To-User</th>\n",
       "      <th>To-User-Id</th>\n",
       "      <th>Language</th>\n",
       "      <th>Source</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geo-Location-Latitude</th>\n",
       "      <th>Geo-Location-Longitude</th>\n",
       "      <th>Retweet-Count</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/5/17 2:03 AM</td>\n",
       "      <td>The Times of London</td>\n",
       "      <td>6.107422e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href='https://www.sprinklr.com' rel='nofoll...</td>\n",
       "      <td>Upgrading to Apple�s latest iPhone operating s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>729.0</td>\n",
       "      <td>9.156535e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/5/17 5:51 AM</td>\n",
       "      <td>hayley from Paramore</td>\n",
       "      <td>4.098180e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href='http://twitter.com/download/iphone' r...</td>\n",
       "      <td>need to feel feelings tonight in the big apple...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>659.0</td>\n",
       "      <td>9.157110e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/4/17 8:00 PM</td>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>3.320479e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href='https://studio.twitter.com' rel='nofo...</td>\n",
       "      <td>*do do do do do do do do* Stranger Things: The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4424.0</td>\n",
       "      <td>9.155623e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/5/17 4:15 PM</td>\n",
       "      <td>julia robert</td>\n",
       "      <td>2.191746e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href='http://twitter.com' rel='nofollow'&gt;Tw...</td>\n",
       "      <td>RT @techbeardblog: The Best iOS 11 Hidden Feat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.158680e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/5/17 4:15 PM</td>\n",
       "      <td>Peggy L Henderson</td>\n",
       "      <td>4.922323e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href='http://www.Feed140.net' rel='nofollow...</td>\n",
       "      <td>Original and enjoyable read. Combination of hi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.158680e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Created-At             From-User  From-User-Id To-User  To-User-Id  \\\n",
       "0  10/5/17 2:03 AM   The Times of London  6.107422e+06     NaN        -1.0   \n",
       "1  10/5/17 5:51 AM  hayley from Paramore  4.098180e+07     NaN        -1.0   \n",
       "2  10/4/17 8:00 PM       Stranger Things  3.320479e+09     NaN        -1.0   \n",
       "3  10/5/17 4:15 PM          julia robert  2.191746e+09     NaN        -1.0   \n",
       "4  10/5/17 4:15 PM     Peggy L Henderson  4.922323e+08     NaN        -1.0   \n",
       "\n",
       "  Language                                             Source  \\\n",
       "0       en  <a href='https://www.sprinklr.com' rel='nofoll...   \n",
       "1       en  <a href='http://twitter.com/download/iphone' r...   \n",
       "2       en  <a href='https://studio.twitter.com' rel='nofo...   \n",
       "3       en  <a href='http://twitter.com' rel='nofollow'>Tw...   \n",
       "4       en  <a href='http://www.Feed140.net' rel='nofollow...   \n",
       "\n",
       "                                                Text  Geo-Location-Latitude  \\\n",
       "0  Upgrading to Apple�s latest iPhone operating s...                    NaN   \n",
       "1  need to feel feelings tonight in the big apple...                    NaN   \n",
       "2  *do do do do do do do do* Stranger Things: The...                    NaN   \n",
       "3  RT @techbeardblog: The Best iOS 11 Hidden Feat...                    NaN   \n",
       "4  Original and enjoyable read. Combination of hi...                    NaN   \n",
       "\n",
       "   Geo-Location-Longitude  Retweet-Count            Id  \n",
       "0                     NaN          729.0  9.156535e+17  \n",
       "1                     NaN          659.0  9.157110e+17  \n",
       "2                     NaN         4424.0  9.155623e+17  \n",
       "3                     NaN           12.0  9.158680e+17  \n",
       "4                     NaN            0.0  9.158680e+17  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(0 = negative, 4 = positive)\n",
    "def predict_sentiment(brand_data_df):\n",
    "    predicted_labels = []\n",
    "    for index, row in brand_data_df.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(\"index:\", index)\n",
    "        text = row['Text']\n",
    "        input_matrix = getSentenceMatrix(text)\n",
    "        predictedSentiment = sess.run(prediction, {input_data: input_matrix})[0]\n",
    "        if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "            #pos\n",
    "            predicted_labels.append(4)\n",
    "            #print(\"Positive Sentiment\")\n",
    "        else:\n",
    "            #neg\n",
    "            predicted_labels.append(0)\n",
    "            #print(\"Negative Sentiment\")\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apple_tweet_df\n",
    "# huawei_tweet_df\n",
    "# oppo_tweet_df\n",
    "# samsung_tweet_df\n",
    "# vivo_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    }
   ],
   "source": [
    "apple_predicted_sentiment = predict_sentiment(apple_tweet_df)\n",
    "apple_tweet_df['sentiment'] = apple_predicted_sentiment\n",
    "apple_tweet_df.to_csv(os.path.join(brand_data_dir, \"Apple_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    }
   ],
   "source": [
    "huawei_predicted_sentiment = predict_sentiment(huawei_tweet_df)\n",
    "huawei_tweet_df['sentiment'] = huawei_predicted_sentiment\n",
    "os.path.join(brand_data_dir, \"Huawei_with_sentiment.csv\")\n",
    "huawei_tweet_df.to_csv(os.path.join(brand_data_dir, \"Huawei_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    }
   ],
   "source": [
    "oppo_predicted_sentiment = predict_sentiment(oppo_tweet_df)\n",
    "oppo_tweet_df['sentiment'] = oppo_predicted_sentiment\n",
    "oppo_tweet_df.to_csv(os.path.join(brand_data_dir, \"Oppo_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    }
   ],
   "source": [
    "samsung_predicted_sentiment = predict_sentiment(samsung_tweet_df)\n",
    "samsung_tweet_df['sentiment'] = samsung_predicted_sentiment\n",
    "os.path.join(brand_data_dir, \"Samsung_with_sentiment.csv\")\n",
    "samsung_tweet_df.to_csv(os.path.join(brand_data_dir, \"Samsung_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    }
   ],
   "source": [
    "vivo_predicted_sentiment = predict_sentiment(vivo_tweet_df)\n",
    "vivo_tweet_df['sentiment'] = vivo_predicted_sentiment\n",
    "vivo_tweet_df.to_csv(os.path.join(brand_data_dir, \"Vivo_with_sentiment.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
